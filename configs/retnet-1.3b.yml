vocab_size: 50257
hidden_size: 2048
num_layers: 24
num_heads: 8
qk_dim: 2048
v_dim: 4096
ffn_proj_size: 4096
use_bias_in_msr: False
use_bias_in_mlp: True
use_bias_in_msr_out: False
use_default_gamma: False
initializer_range: 0.02
output_retentions: False
pad_token_id: 50256
eos_token_id: 50256
unk_token_id: 50256
